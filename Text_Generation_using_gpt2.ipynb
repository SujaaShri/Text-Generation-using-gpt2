{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "tcwa1fx7WE2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6iQlKNqnqjU"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "POMLPgH7n8pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 example prompts\n",
        "prompt_1=\"It was a bright and sunny\"\n",
        "prompt_2=\"She opened the book and\""
      ],
      "metadata": {
        "id": "40NnY_TZoFCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 tokenize the prompts\n",
        "ids1=tokenizer(prompt_1,return_tensors=\"pt\").input_ids\n",
        "ids2=tokenizer(prompt_2,return_tensors=\"pt\").input_ids\n",
        "print(\"Prompt 1:\")\n",
        "print(ids1)\n",
        "print(\"\\nPrompt 2:\")\n",
        "print(ids2)"
      ],
      "metadata": {
        "id": "FShkByB2oOac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 Show the decoded raw token ids and their decoded tokens for prompt 1 & 2\n",
        "output_ids1=gpt2.generate(ids1,max_new_tokens=20)\n",
        "print(\"Prompt 1:\\nRaw token ids:\")\n",
        "for t in ids1[0]:\n",
        "  print(t,\":\\t\",tokenizer.decode(t))\n",
        "print(\"\\nDecoded token ids:\")\n",
        "print(output_ids1)\n",
        "print(\"\\nDecoded text:\")\n",
        "tokenizer.decode(output_ids1[0])\n"
      ],
      "metadata": {
        "id": "7Ee66TyVtOb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_ids2=gpt2.generate(ids2,max_new_tokens=20)\n",
        "print(\"\\nPrompt 2:\\nRaw token ids:\")\n",
        "for t in ids2[0]:\n",
        "  print(t,\":\\t\",tokenizer.decode(t))\n",
        "print(\"\\nDecoded token ids:\")\n",
        "print(output_ids2)\n",
        "print(\"\\nDecoded text:\")\n",
        "tokenizer.decode(output_ids2[0])"
      ],
      "metadata": {
        "id": "TKXqGH2BlP9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Sampling Techniques\n",
        "print(\"Prompt 1:\")\n",
        "# Temperature Sampling (temperature = 0.7)\n",
        "print(\"\\nTemperature Sampling (temperature = 0.7)\")\n",
        "output_temp = gpt2.generate(\n",
        "    ids1,\n",
        "    max_new_tokens=20,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_k=0,\n",
        "    top_p=1.0\n",
        ")\n",
        "print(tokenizer.decode(output_temp[0], skip_special_tokens=True))\n",
        "\n",
        "# Top-k Sampling (top_k = 50)\n",
        "print(\"\\nTop-k Sampling (top_k = 50)\")\n",
        "output_topk = gpt2.generate(\n",
        "    ids1,\n",
        "    max_new_tokens=20,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=1.0,\n",
        "    temperature=1.0\n",
        ")\n",
        "print(tokenizer.decode(output_topk[0], skip_special_tokens=True))\n",
        "\n",
        "# Top-p (Nucleus) Sampling (top_p = 0.9)\n",
        "print(\"\\nTop-p (Nucleus) Sampling (top_p = 0.9)\")\n",
        "output_topp = gpt2.generate(\n",
        "    ids1,\n",
        "    max_new_tokens=20,\n",
        "    do_sample=True,\n",
        "    top_k=0,\n",
        "    top_p=0.9,\n",
        "    temperature=1.0\n",
        ")\n",
        "print(tokenizer.decode(output_topp[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "uy9dW-TvReOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Prompt 2:\")\n",
        "# Temperature Sampling (temperature = 0.7)\n",
        "print(\"\\nTemperature Sampling (temperature = 0.7)\")\n",
        "output_temp = gpt2.generate(\n",
        "    ids2,\n",
        "    max_new_tokens=20,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_k=0,\n",
        "    top_p=1.0\n",
        ")\n",
        "print(tokenizer.decode(output_temp[0], skip_special_tokens=True))\n",
        "\n",
        "# Top-k Sampling (top_k = 50)\n",
        "print(\"\\nTop-k Sampling (top_k = 50)\")\n",
        "output_topk = gpt2.generate(\n",
        "    ids2,\n",
        "    max_new_tokens=20,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=1.0,\n",
        "    temperature=1.0\n",
        ")\n",
        "print(tokenizer.decode(output_topk[0], skip_special_tokens=True))\n",
        "\n",
        "# Top-p (Nucleus) Sampling (top_p = 0.9)\n",
        "print(\"\\nTop-p (Nucleus) Sampling (top_p = 0.9)\")\n",
        "output_topp = gpt2.generate(\n",
        "    ids2,\n",
        "    max_new_tokens=20,\n",
        "    do_sample=True,\n",
        "    top_k=0,\n",
        "    top_p=0.9,\n",
        "    temperature=1.0\n",
        ")\n",
        "print(tokenizer.decode(output_topp[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "rm-Gx_gCTDSc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}