# Text-Generation-using-gpt2

This is a simple mini project demonstrating text generation using the GPT-2 model from Hugging Face Transformers. The notebook explores how different sampling techniques affect the generated output.


## Technologies Used

- Python
- Hugging Face Transformers
- PyTorch
- Google Colab

## How to Run

1. Open the notebook in [Google Colab](https://colab.research.google.com/)
2. Run all cells sequentially
3. Review generated outputs for each decoding method

## Prompts Used

- "It was a bright and sunny"
- "She opened the book and"
